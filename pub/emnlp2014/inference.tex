\Section{inference}{Inference As Search}
%In a classical inference setting, a system is provided both the
%  query consequent, and a set of antecedents from which to derive
%  the validity of the consequent.
%Prior work on inference with Natural Logic has assumed this scenario
%  \cite{key:2008maccartney-natlog}, and taken the two-step of approach
%  of aligning the antecedent and consequent, and classifying each
%  aligned segment into a mutation relation.

%For common-sense reasoning, we are not given a well-defined
%  antecedent, but rather have at our disposal a large database of
%  true facts.
%This makes the align-and-classify approach of 
%  \refsec{maccartney-proof} substantially
%  less appealing, as candidate antecedents are not readily available.

Natural Logic allows us to formalize our approach elegantly as
  a single search problem.
Given a query, we search over the space of possible facts for
  a valid premise in our database.
The nodes in our search problem correspond to candidate facts
  (\refsec{inference-state});
  the edges are mutations of these facts (\refsec{inference-transitions}); 
  the costs over these edges encode the confidence that this edge
  maintains an informative inference (\refsec{inference-prob}).
This mirrors the automata defined in \refsec{search}, except importantly
  we are constructing a reversed derivation, and are therefore
  ``traversing'' the FSA backwards.

This approach is efficient over a large database of 270 million entries
  without making use of explicit queries over the database;
  nor does the approach make use of any sort of approximate
  matching against the database, beyond lemmatizing individual 
  lexical items.
The motivation in prior work for approximate matches --
  to improve the recall of candidate premises -- is captured
  elegantly by relaxing Natural Logic itself.
%However, occasionally approximate matches, penalized appropriately,
%  are beneficial for downstream applications.
We show that allowing invalid transitions with appropriate costs
  generalizes JC distance \cite{key:1997jc-similarity} -- a common
  thesaurus-based similarity metric (\refsec{inference-jc}).
Importantly, however, the entire inference pipeline is done
  within the framework of weighted lexical transitions in Natural
  Logic.

%We define the problem by specifying the state space,
%  the valid transitions, along with the weights of
%  the transitions.
%We then describe how these weights translate to the confidence of truth
%  of a fact, and show that our search is at worst a generalization of
%  JC distance \cite{key:1997jc-similarity} -- a common WordNet similarity
%  metric.

%
% Node
%
\Subsection{inference-state}{Nodes}
The space of possible nodes in our search is the set of possible
  partial derivations.
To a first approximation, this is a pair $(w, s)$ of a surface form
  $w$ tagged with word sense and polarity,
  and an inference state $s \in \{\textrm{valid}, \textrm{invalid}\}$
  in our collapsed FSA (\reffig{fsa}b).
For example, the search path in \reffig{teaser} traverses the nodes:

\vspace{0.5em}
\begin{tabular}{lr}
  (\w{No carnivores eat animals}, & \textrm{valid}) \\
  (\w{The carnivores eat animals}, & \textrm{invalid}) \\
  (\w{The cat eats animals}, & \textrm{invalid}) \\
  (\w{The cat eats an animal}, & \textrm{invalid}) \\
  (\w{The cat ate a mouse}, & \textrm{invalid}) \\
\end{tabular}
\vspace{0.5em}

During search, we assume that the validity states $s$ are reversible --
  if we know that \textit{the cat ate a mouse} is true, we can
  infer that \textit{no carnivores eat animals} is false.
In addition, our search keeps track of some additional information:

%That is, a node is primarily parametrized primarily by the surface
%  form of the candidate fact at this point in the (reverse) derivation.
%
%This surface form is augmented with both sense and polarity
%  information.
%Each word is allowed a sense (up to 31 possible senses), or a default
%  sense; in addition, 2 bits are reserved for marking polarity
%  (monotone, antitone, non-monotone).
%This leaves sufficient space to store each augmented word in a 32 bit
%  integer.
%Note also that we refer to potentially arbitrary lexical items in
%  WordNet as words -- for instance, \textit{fire truck} is treated as
%  a single word.

\paragraph{Mutation Index}
Edges are most naturally defined to correspond to mutations of 
  individual lexical items.
We maintain an index of the next item to mutate at each search state.
Importantly, this enforces that each derivation orders mutations
  left-to-right;
  this is computationally efficient, at the expense of rare search
  errors.
A similar observation is noted in \newcite{key:2009maccartney-thesis},
  where prematurely collapsing to \independent\ occasionally misses
  inferences.

\paragraph{Polarity}
Mutating quantifiers can change the polarity information on a span
  in the fact.
Since we do not have the full parse tree at our disposal at search,
  we track a small amount of metadata to guess the scope of the
  mutated quantifier.

%As per our definition, transitions between nodes are transitions 
%  between facts.
%Lexical resources for mutations are over lexical items
%  (e.g., \textit{feline} $\reverse$ \textit{cat}) rather than
%  entire facts.
%This motivates the inclusion of an index $i$ in our node, denoting the
%  index of the item which may be mutated.
%Importantly, this imposes a natural ordering of items to
%  mutate, making search more efficient at the expense of rare
%  search errors.
%The mutation index is allowed to shift forwards through the
%  fact (at no cost), but not backwards.
%This helps make search more tractable, at the expense of rare
%  esoteric search errors.

%
% TRANSITIONS
%
\Subsection{inference-transitions}{Transitions}
We begin by introducing some terminology.
A \textit{transition template} is a broad class of transitions; for
  instance WordNet hypernymy.
A \textit{transition} (or \textit{transition instance}) is a particular
  instantiation of a transition template.
For example, the transition from \textit{cat} to \textit{feline}.
Lastly, an \textit{edge} in the search space connects two nodes, which
  are separated by a single transition instance.
For example, an edge exists between 
  \textit{some felines have tails} and \textit{some cats have tails}.
Transition \lbrack instances\rbrack\ are stored statically in memory, 
  whereas edges are constructed on demand.

Transition templates provide a means of defining transitions and
  subsequently edges in our search space using existing lexical
  resources (e.g., WordNet, distributional similarity, etc.).
We can then define a mapping from these templates to Natural Logic
  lexical relations.
This allows us to map every edge in our search graph back to the
  Natural Logic relation it instantiates.
%Note that the edges in the search are not constructed \textit{a priori}.
%It is sufficient to store the transitions, and construct particular
%  edges on demand.
The full table of transition templates is given in \reftab{transitions},
  along with the Natural Logic relation 
  that instances of the template introduce.
We include most relations in WordNet as transitions,
  and parametrize insertions and deletions by the part of speech
  of the token being inserted/deleted.

Once we have an edge defining a lexical mutation with
  an associated Natural Logic relation $r$, we can construct the 
  corresponding end node
  $(w', s')$ such that $w'$ is the sentence with the lexical mutation
  applied, and $s'$ is the validity state obtained from the
  FSA in \refsec{search}.
For instance, if our edge begins at $(w, s)$, and there exists
  a transition in the FSA from $s' \xrightarrow{r} s$, then we
  define the end point of the edge to be $(w', s')$.
To illustrate concretely, suppose our search state is:

\vspace{0.5em}
\begin{tabular}{lr}
(\textit{some felines have tails}, & valid)
\end{tabular}
\vspace{0.5em}

The transition template for WordNet hypernymy gives us a transition
  instance from \textit{feline} to \textit{cat}, corresponding to the
  Natural Logic inference 
  $\textit{cat} \xrightarrow{\forward} \textit{feline}$.
Recall, we are constructing the inference in reverse, starting from
  the consequent (query).
We then notice that the transition 
  $valid \xrightarrow{\forward} valid$ in the FSA ends in our current
  inference state (\textit{valid}), and set our new inference state to 
  be its start -- in this case, we maintain validity.

Note that negation is somewhat subtle, as the transitions are not
  symmetric from valid to invalid and visa versa, and we do not know
  our true inference state with respect to the premise
  yet.
In practice, the search procedure treats all three of
  $\{\negate, \alternate, \cover\}$ as negation, and re-scores complete
  derivations once their inference states are known.

\begin{table}
\begin{center}

%  \begin{tabular}{lcl}
%    \textbf{Template} & \textbf{Relation} & \textbf{Example edge} \\
%    \hline
%    WordNet hypernym                     & \forward    & \textit{some cats like milk} \forward\ \textit{some felines like milk} \\
%    WordNet hyponym                      & \reverse    & \textit{some felines like milk} \reverse\ \textit{some cats like milk} \\
%    WordNet antonym$^\dagger$            & \alternate  & \textit{all cats like milk} \alternate\ \textit{all cats hate milk} \\
%    WordNet synonym/pertainym$^\dagger$  & \equivalent & \textit{some cats like milk} \equivalent\ \textit{some cats enjoy milk} \\
%    Distributional nearest neighbor      & \equivalent & \textit{some cats like milk} \equivalent\ \textit{some cats like dairy} \\
%    Delete word$^\dagger$                & \forward    & \textit{some tabby cats like milk} \forward\ \textit{some cats like milk} \\
%    Add word$^\dagger$                   & \reverse    & \textit{some cats like milk} \reverse\ \textit{some tabby cats like milk} \\
%    Quantifier weaken                    & \forward    & \textit{all cats like milk} \forward\ \textit{some cats like milk} \\
%    Quantifier strengthen                & \reverse    & \textit{some cats like milk} \reverse\ \textit{all felines like milk} \\
%    Quantifier negate                    & \negate     & \textit{some cats like milk} \negate\ \textit{no felines like milk} \\
%    Quantifier synonym                   & \equivalent & \textit{some cats like milk} \equivalent\ \textit{a few cats like milk} \\
%    Change word sense                    & \equivalent & 
%  \end{tabular}
  
  \begin{tabular}{lcl}
    \textbf{Transition Template} & \textbf{Relation} \\
    \hline
    WordNet hypernym                     & \forward    \\ 
    WordNet hyponym                      & \reverse    \\ 
    WordNet antonym$^\dagger$            & \alternate  \\ 
    WordNet synonym/pertainym$^\dagger$  & \equivalent \\ 
    Distributional nearest neighbor      & \equivalent \\ 
    Delete word$^\dagger$                & \forward    \\ 
    Add word$^\dagger$                   & \reverse    \\ 
    Quantifier weaken                    & \forward    \\ 
    Quantifier strengthen                & \reverse    \\ 
    Quantifier negate                    & \negate     \\ 
    Quantifier synonym                   & \equivalent \\ 
    Change word sense                    & \equivalent \\ 
  \end{tabular}
	\caption{
    The edges allowed during inference.
    Entries with a dagger ($\dagger$) are parametrized by their part-of-speech
      tag, from the restricted list of $\{$noun$,$adjective$,$verb$,$other$\}$.
    The first column describes the type of the transition.
    The set-theoretic relation introduced by each relation is given in
      the second column.
%    The third column gives an example of the transition in practice,
%      as an edge in the search graph.
		\label{tab:transitions}
	}
\end{center}
\end{table}

It should be noted that the mapping from transition templates to relation
  types is intentionally imprecise.
For instance, clearly nearest neighbors do not preserve equivalence
  (\equivalent); more subtly, while
  \textit{all cats like milk} \alternate\ \textit{all cats hate milk},
  it is not the case that
  \textit{some cats like milk} \alternate\ \textit{some cats hate milk}.\footnote{
    The latter example is actually a consequence of the projection function in
    \reftab{projectivity} being overly optimistic.
  }
We mitigate this imprecision by introducing a cost for each transition,
  and learning the appropriate value for this cost
  (see \refsec{learning}).
The cost of an edge
  from fact $(w,v)$ with surface form $w$ and validity
  $v$ to a new fact $(w',v')$, using a transition
  instance $t_i$ of template $t$ and mutating a word
  with polarity $p$, is given by
  $f_{t_i} \cdot \theta_{t,v,p}$.
We define this as:

\vspace{-0.25em}
\begin{itemize}
  \setlength{\itemsep}{-0.25em}
  \indentitem\item[$f_{t_i}$:]
    A value associated with every transition instance $t_i$, intuitively
      corresponding to how ``far'' the endpoints of the transition are.
  \indentitem\item[$\theta_{t,v,p}$:]
    A learned cost for taking a transition of template $t$, if the source
    of the edge is in a inference state of $v$ and the word being mutated
    has polarity $p$.
\end{itemize}
\vspace{-0.25em}

The notation for $f_{t_i}$ is chosen to evoke an analogy to features.
We set $f_{t_i}$ to be 1\ in most cases;
  the exceptions are the edges over the WordNet hypernym tree
  and the nearest neighbors edges.
In the first case, taking the hypernymy relation
  from $w$ to $w'$ as $\uparrow_{w \rightarrow w'}$, we set:
%  (and vice versa for
%  $\downarrow_{w \rightarrow w'}$), and set:

\vspace{-0.5em}
\begin{equation*}
  f_{\uparrow_{w \rightarrow w'}}   = \log \frac{p(w')}{p(w)} = \log p(w') - \log p(w).
%  f_{\downarrow_{w \rightarrow w'}} &= \log \frac{p(w)}{p(w')} &= \log p(w) - \log p(w')
\end{equation*}
\vspace{-0.5em}

We define $p(w)$ to be the ``probability'' of a concept --
  that is, the normalized frequency
  of a word $w$ or any of its hyponyms in the Google N-Grams corpus
  \cite{key:2006brants-ngrams}.
Intuitively, this ensures that relatively long paths through fine-grained
  sections of WordNet are not unduly penalized.
For instance, the path from \textit{cat} to \textit{animal} traverses
  six intermediate nodes, \naive ly yielding a prohibitive
  search depth of 6.
However, many of these transitions have low weight:
  for instance
  $f_{\uparrow_{\textit{cat} \rightarrow \textit{feline}}}$ is only
  \num{0.37}.
The value $f_{\downarrow_{w \rightarrow w'}}$ is set analogously.

For nearest neighbors edges, we take Neural Network embeddings learned
  in \newcite{key:huang-vectors} corresponding to each vocabulary entry.
We then define $f_{NN_{w \rightarrow w'}}$
  to be the arc cosine of the cosine similarity (i.e., the angle)
  between word vectors associated with lexical items $w$ and $w'$:

\vspace{-0.5em}
\begin{equation*}
  f_{NN_{w \rightarrow w'}}
    = \textrm{arccos} \left( \frac{w \cdot w'}{\|w\| \|w'\|} \right).
\end{equation*}
\vspace{-0.5em}

For instance, $f_{NN_{\textit{cat} \rightarrow \textit{dog}}} = 0.43$.
In practice, we explore the 100 nearest neighbors for each word.

We can express $f_{t_i}$ as a feature vector by representing it as
  a vector with value $f_{t_i}$ at the index corresponding to
  $(t,v,p)$ -- the transition template, the validity of the inference,
  and the polarity of the mutated word.
Note that the size of this vector mirrors the number of cost
  parameters $\theta_{t,v,p}$, and is in general smaller than the
  number of transition instances.

A search path can then be parametrized by a sequence of feature vectors
  $f_1, f_2, \dots, f_n$, which in turn can be collapsed into a single
  vector $\bef = \sum_i f_i$.
The cost of a path is defined as $\theta \cdot \bef$, where
  $\theta$ is the vector of $\theta_{t,v,p}$ values.
Both $\bef$ and $\theta$ are constrained to be non-negative, or else
  the search problem is misspecified.

%The set of features which fire along a path
%  can be expressed as a vector $\bef$.
%Each element of $\bef$ corresponds to the sum of the values of that
%  feature along the path.
%Correspondingly, we define the weight vector $\theta$ as the
%  weight for every element of $\bef$.
%The cost of a path can then be expressed as the dot product:
%  $\theta \cdot \bef$.

%
% Generalize JC
%
\Subsection{inference-jc}{Generalizing Similarities}
An elegant property of our definitions of $f_{t_i}$ is its ability to
  generalize JC distance.
Let us assume we have lexical items $w_1$ and $w_2$, with a least common 
  subsumer $\textrm{lcs}$.
The JC distance $\textrm{dist}_{\textrm{jc}}(w_1, w_2)$ is:

\vspace{-0.5em}
\begin{equation}
\textrm{dist}_{\textrm{jc}}(w_1, w_2)
  = \log\frac{p(\textrm{lcs})^2}{p(w_1) \cdot p(w_2)}.
\label{eqn:jc}
\end{equation}
\vspace{-0.5em}

For simplicity, we simplify $\theta_{\uparrow,v,p}$ and $\theta_{\downarrow,v,p}$
  as simply $\theta_\uparrow$ and $\theta_\downarrow$.
%The derivation generalizes trivially to the the case where weights are
%  further parametrized.
Without loss of generality, we also assume that a path in our search
  is only modifying a single lexical item $w_1$, eventually 
  reaching a mutated form $w_2$.

We can factorize the cost of a path, $\theta \cdot \bef$, along the path
  from $w_1$ to $w_2$ through its lowest common subsumer (lcs),
  $[w_1, w_1^{(1)}, \dots, \textrm{lcs}, \dots,  w_2^{(1)}, w_2]$,
  as follows:

\vspace{-0.5em}
\begin{align*}
\theta \cdot \phi
  &= \theta_\uparrow \left( 
    \left[\log p(w_1^{(1)}) - \log p(w_1)\right] +
    \dots
%   + \left[\log p(w_1^{(n)}) - \log p(\textrm{lcs})\right]
    \right) + \\
  &~~~~~~ \theta_\downarrow \left( 
    \left[\log p(\textrm{lcs}) - \log p(w_1^{(n)}) \right] +
    \dots
%    + \left[\log p(w_1) - \log p(w_1^{(1)})\right]
    \right) \\
  &= \theta_\uparrow \left( \log \frac{p(\textrm{lcs})}{p(w_1)} \right) +
     \theta_\downarrow \left( \log \frac{p(\textrm{lcs})}{p(w_2)} \right) \\
  &= \log \frac{ p(\textrm{lcs})^{\theta_\uparrow + \theta_\downarrow} }
               { p(w_1)^{\theta_\uparrow} \cdot p(w_2)^{\theta_\downarrow} }.
\end{align*}
\vspace{-0.5em}

Note that setting both $\theta_\uparrow$ and $\theta_\downarrow$ to 1 exactly
  yields Formula \refeqn{jc} for JC distance.
This, in addition to the inclusion of nearest neighbors as transitions,
  allows the search to capture the intuition that similar objects
  have similar properties
  (e.g., as used in \newcite{key:2013angeli-truth}).
%
%It is also worth noting that the nearest neighbors path provides an
%  upper bound on the true similarity between the start and end item
%  in the path.
%In this way, the search based approach presented here can be thought
%  of as generalizing and formalizing the intuition that similar objects 
%  have similar properties (e.g., as presented in
%  \newcite{key:2013angeli-truth}) using both common classes of similarity
%  metrics.


%
% PROBABILITY
%
\Subsection{inference-deletions}{Deletions in Inference}
Although inserting lexical items in a derivation (deleting words from
  the reversed derivation) is trivial, the other direction is not.
For brevity, we refer to a deletion in the derivation as an insertion,
  since from the perspective of search we are inserting lexical items.

\Naive ly, at every node in our search we must consider every item in
  the vocabulary as a possible insertion.
We can limit the number of items we consider by storing the database
  as a trie.
Since the search mutates the fact left-to-right 
  (as per \refsec{inference-state}), we can
  consider children of a trie node as candidate insertions.
To illustrate, given a search state with fact $w_0\,w_1 \dots w_n$
  and mutation index $i$, we would look up completions $w_{i+1}$ for
  $w_0\,w_1 \dots w_i$ in our trie of known facts.

Although this approach works well when $i$ is relatively large, there
  are too many candidate insertions for small $i$.
We special case the most extreme example for this, where $i=0$ -- that is, 
  when we are inserting into the beginning of the fact.
In this case, rather than taking all possible lexical items that start
  any fact, we take all items which are followed by the first word of
  our current fact.
To illustrate, given a search state with fact $w_0\,w_1 \dots w_n$,
  we would propose candidate insertions $w_{-1}$ such that
  $w_{-1}\,w_0\,w^\prime_1 \dots w^\prime_k$ is a known fact
  for some $w^\prime_1 \dots w^\prime_k$.
More concretely, if we know that \w{fluffy cats have tails}, and are
  at a node corresponding to \w{cats like boxes}, we propose \w{fluffy}
  as a possible insertion: \w{fluffy cats like boxes}.

%
% PROBABILITY
%
\Subsection{inference-prob}{Confidence Estimation}
The last component in inference is translating a search path into a
  probability of truth.
We notice from \refsec{inference-transitions} that the \textit{cost}
  of a path can be represented as $\theta \cdot \bef$.
We can normalize this value by negating every element of the cost
  vector $\theta$ and passing it through a sigmoid:

\vspace{-0.5em}
\begin{equation*}
\textrm{confidence} = \frac{1}{1 + e^{- (-\theta \cdot \bef)}}.
\end{equation*}
\vspace{-0.5em}

Importantly, note that the cost vector must be non-negative for the
  search to be well-defined, and therefore the confidence value will
  be constrained to be between 0 and $\frac{1}{2}$.

At this point, we have a confidence that the given path has not violated
  strict Natural Logic.
However, to translate this value into a probability
  we need to incorporate whether the inference path is
  confidently valid, or confidently invalid.
To illustrate, a fact with a low confidence should translate to a
  probability of $\frac{1}{2}$, rather than a probability of 0.
We therefore define the probability of validity as follows:
We take $v$ to be 1 if the final fact of our derivation is in the
  \textit{valid} state with respect to the antecedent,
  and $-1$ if this fact is in the \textit{invalid} state.
%In practice, as our search is reversed, we take the state of the
%  antecedent in our database when compared to the consequent, rather
%  than vice versa.
For completeness, if no path is given we can set $v=0$.
The probability of validity becomes:

\vspace{-0.5em}
\begin{equation}
  p(\textrm{valid}) = \frac{v}{2} + \frac{1}{1 + e^{v \theta \cdot \bef}}.
  \label{eqn:prob}
\end{equation}
\vspace{-0.5em}

Note that in the case where $v=-1$, the above expression reduces to
  $\frac{1}{2} - \textrm{confidence}$; in the case where $v=0$ it
  reduces to simply $\frac{1}{2}$.
Furthermore, note that the probability of truth makes use of the same
  parameters as the cost in the search.
%Thus, as better weights are learned, the search is likewise more likely
%  to produce derivations which would confidently support or disprove the
%  query.
%We proceed to describe how these weights are learned.
